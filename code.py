!pip install langchain chromadb sentence-transformers groq streamlit pypdf python-dotenv tqdm

import os

from google.colab import userdata



# Use Colab's Secrets manager to store your API key

# Click on the "ðŸ”‘" icon in the left sidebar, add a new secret named GROQ_API_KEY

os.environ["GROQ_API_KEY"] = userdata.get('GROQ_API_KEY')



# You can optionally print to confirm it's loaded (be careful not to display the key itself)

# print("GROQ_API_KEY loaded.")

!pip install -U langchain-community

!pip install langchain-groq

from langchain_groq import ChatGroq

import os



groq_api_key = os.getenv("GROQ_API_KEY")



if not groq_api_key:

Â  Â  print("Error: GROQ_API_KEY not found in environment variables. ChatGroq model not initialized.")

Â  Â  llm = None

else:

Â  Â  print("GROQ_API_KEY found in environment variables.")

Â  Â  llm = ChatGroq(

Â  Â  Â  Â  model="llama3-70b-8192",

Â  Â  Â  Â  temperature=0,

Â  Â  Â  Â  api_key=groq_api_key # Pass the key explicitly

Â  Â  )

Â  Â  print("ChatGroq model initialized.")



#FOR PDF DATA EXTRACTION, HERE'S THE CODE â¬‡



JUST UPLOAD YOUR PDF AND COPY ITS PATH IN THE "pdf_path" AND THERE YOU GO.

from pypdf import PdfReader



# Replace 'your_document.pdf' with the actual path to your PDF file

pdf_path = r"/content/REPORTSAHIL.pdf"

raw_text = ''

try:

Â  Â  reader = PdfReader(pdf_path)

Â  Â  for page in reader.pages:

Â  Â  Â  Â  raw_text += page.extract_text() + "\n"

Â  Â  print("Text extracted successfully!")

except FileNotFoundError:

Â  Â  print(f"Error: The file {pdf_path} was not found.")

except Exception as e:

Â  Â  print(f"An error occurred: {e}")



# Now you can use this 'raw_text' in the subsequent steps of your QA system

# For example, you would use it when creating the Chroma vector DB:

# vectordb = Chroma.from_texts(chunks, embedding=embedding_model, persist_directory="chroma_store")



from langchain.text_splitter import RecursiveCharacterTextSplitter



text_splitter = RecursiveCharacterTextSplitter(

Â  Â  chunk_size=1000,

Â  Â  chunk_overlap=200

)

chunks = text_splitter.create_documents([raw_text])

print(f"Created {len(chunks)} chunks.")



from langchain_community.embeddings import SentenceTransformerEmbeddings



embedding_model = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")



from langchain_community.vectorstores import Chroma



vectordb = Chroma.from_documents(

Â  Â  documents=chunks,

Â  Â  embedding=embedding_model,

Â  Â  persist_directory="chroma_store"

)

print("Chroma vector database created and persisted.")



from langchain.chains import RetrievalQA



retriever = vectordb.as_retriever()



qa_chain = RetrievalQA.from_chain_type(

Â  Â  llm=llm,

Â  Â  retriever=retriever,

Â  Â  chain_type="stuff"

)

print("RetrievalQA chain created.")



query = "What does the pdf shows?"

result = qa_chain.run(query)

print("\nAnswer:", result)

# Assuming qa_chain is already initialized from previous steps

if 'qa_chain' in locals() and qa_chain is not None:

Â  Â  query = "Provide a detailed description of the content of the PDF file."

Â  Â  try:

Â  Â  Â  Â  result = qa_chain.run(query)

Â  Â  Â  Â  print("\nDescription of the PDF file:", result)

Â  Â  except Exception as e:

Â  Â  Â  Â  print(f"An error occurred while getting the PDF description: {e}")

else:

Â  Â  print("QA chain is not initialized. Please run the necessary cells to set up the QA chain.")



import os

from groq import Groq

from google.colab import userdata



# Load the API key from Colab Secrets

groq_api_key = userdata.get('GROQ_API_KEY')



if not groq_api_key:

Â  Â  print("Error: GROQ_API_KEY not found in Colab Secrets.")

else:

Â  Â  try:

Â  Â  Â  Â  # Initialize the Groq client directly

Â  Â  Â  Â  client = Groq(api_key=groq_api_key)



Â  Â  Â  Â  # Make a simple API call to test authentication

Â  Â  Â  Â  chat_completion = client.chat.completions.create(

Â  Â  Â  Â  Â  Â  messages=[

Â  Â  Â  Â  Â  Â  Â  Â  {

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "role": "user",

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "content": "Hello",

Â  Â  Â  Â  Â  Â  Â  Â  }

Â  Â  Â  Â  Â  Â  ],

Â  Â  Â  Â  Â  Â  model="llama3-8b-8192", # Using a smaller, faster model for a quick test

Â  Â  Â  Â  )



Â  Â  Â  Â  print("Direct Groq API call successful!")

Â  Â  Â  Â  print("Response:", chat_completion.choices[0].message.content)



Â  Â  except Exception as e:

Â  Â  Â  Â  print(f"Error during direct Groq API call: {e}")

Â  Â  Â  Â  print("This error likely indicates an issue with the API key loaded from Secrets.")



#HERE'S THE AI CLONE â¬‡



JUST ASK ANYHTING FROM WRITING THE CODE TO GET YOU A STORY, ANYTHING IN JUST ONE PROMPT

import os

from langchain_groq import ChatGroq

from google.colab import userdata

from datetime import datetime # Import datetime to get both date and time

from datetime import date # Still need date for date.today() if desired, but datetime.now() is sufficient



# Ensure GROQ_API_KEY is loaded from Secrets

groq_api_key = userdata.get('GROQ_API_KEY')



if not groq_api_key:

Â  Â  print("Error: GROQ_API_KEY not found in Colab Secrets.")

Â  Â  llm = None

else:

Â  Â  # Initialize the ChatGroq model

Â  Â  # You can choose a different model if available

Â  Â  llm = ChatGroq(model="llama3-70b-8192", temperature=0, api_key=groq_api_key)

Â  Â  print("ChatGroq model initialized.")



if llm:

Â  Â  while True:

Â  Â  Â  Â  # Ask a general question

Â  Â  Â  Â  general_query = input("Enter your question (or type 'quit' to exit): ")



Â  Â  Â  Â  if general_query.lower() == 'quit':

Â  Â  Â  Â  Â  Â  print("Thank you for chatting! Goodbye.")

Â  Â  Â  Â  Â  Â  break



Â  Â  Â  Â  if general_query.strip(): # Only process if the query is not empty

Â  Â  Â  Â  Â  Â  # Get current date and time

Â  Â  Â  Â  Â  Â  now = datetime.now()

Â  Â  Â  Â  Â  Â  date_time_info = f"Current date and time is {now}. "



Â  Â  Â  Â  Â  Â  # Check if the query is a simple greeting (case-insensitive, leading/trailing spaces ignored)

Â  Â  Â  Â  Â  Â  simple_greetings = ["hi", "hello", "hey"]

Â  Â  Â  Â  Â  Â  if general_query.strip().lower() in simple_greetings:

Â  Â  Â  Â  Â  Â  Â  Â  # If it's a simple greeting, send the original query without date/time

Â  Â  Â  Â  Â  Â  Â  Â  prompt_to_send = general_query

Â  Â  Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  Â  Â  # For other queries, combine date/time info with the user's query

Â  Â  Â  Â  Â  Â  Â  Â  prompt_to_send = date_time_info + general_query



Â  Â  Â  Â  Â  Â  try:

Â  Â  Â  Â  Â  Â  Â  Â  # Get the response from the model

Â  Â  Â  Â  Â  Â  Â  Â  response = llm.invoke(prompt_to_send) # Use the constructed prompt



Â  Â  Â  Â  Â  Â  Â  Â  print("\nQuestion:", general_query) # Print original question

Â  Â  Â  Â  Â  Â  Â  Â  print("Answer:", response.content)

Â  Â  Â  Â  Â  Â  except Exception as e:

Â  Â  Â  Â  Â  Â  Â  Â  print(f"An error occurred during the API call: {e}")

Â  Â  Â  Â  else:

Â  Â  Â  Â  Â  Â  print("Please enter a question.")



Â  Â  Â  Â  print("-" * 30) # Separator for clarity



else:

Â  Â  print("LLM model not initialized due to missing API key. Cannot proceed.")
